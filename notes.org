
* Changes

- Add torch.cuda.synchronize() around regions ([Mar 17 2021]: made configurable)
- Annotate regions for the torch autograd profiler: this works well with [[https://github.com/indigoviolet/torchprof][torchprof]]
- Added MemoryClock() to measure memory usage


* TODO

1. [X] add type hints and make it PEP-561 compliant (py.typed)
2. Get rid of actual profiler stuff and merge into torchprof

   ^ wait: region_profiler might actually be useful for memory profiling

   Also note: torch profiler already has tensor-specific memory profiling, so
   this would add global memory profiling

3. Get memory usage to be printed as ~MB~ etc  instead of as ~s~
4. Choose root node instead of using the node from \.install()
5. don't use =atexit=, instead use a context manager (same as 4.)


* Notes

** Synchronization, multiprocessing, dataset

Seems like adding @rp.func() to ~__getitem__~ on a dataset that uses
multiprocessing will lead to some warnings about not being able to synchronize
in a forked multiprocessing process.

They recommend changing the multiprocessing start method, but this makes things slower

#+BEGIN_SRC python
import multiprocessing
multiprocessing.set_start_method('forkserver', force=True) # or 'spawn'
#+END_SRC

Is it possible to not synchronize here and still get correct results somehow?
